{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a446cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\velch\\anaconda3\\lib\\site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from selenium) (2023.5.7)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\velch\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\velch\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\velch\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\velch\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\velch\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Selenium.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba4fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea22b12",
   "metadata": {},
   "source": [
    "Q1: In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter. \n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results. \n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    " The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs \n",
    "The task will be done as shown in the below steps:\n",
    " 1. first get the web page https://www.naukri.com/ \n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    " 3. Then click the search button.\n",
    " 4. Then apply the location filter and salary filter by checking the respective boxes\n",
    " 5. Then scrape the data for the first 10 jobs results you get. \n",
    "6. Finally create a dataframe of the scraped data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa3a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6547a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7772ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()\n",
    "time.sleep(5)\n",
    "location_filter=driver.find_element(By.XPATH,\"/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[5]/div[2]/div[3]/label/p/span[1]\")\n",
    "location_filter.click()\n",
    "time.sleep(5)\n",
    "salary_filter=driver.find_element(By.XPATH,\"/html/body/div/div/main/div[1]/div[1]/div/div/div[2]/div[6]/div[2]/div[2]/label/p/span[1]\")\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fade3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "job_company=[]\n",
    "job_experience=[]\n",
    "job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786b4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags=driver.find_elements(By.XPATH,'//div[@class=\" row1\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ni-job-tuple-icon ni-job-tuple-icon-srp-location loc\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\" comp-name mw-25\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    job_company.append(company)\n",
    "    \n",
    "\n",
    "experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"ni-job-tuple-icon ni-job-tuple-icon-srp-experience exp\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    job_experience.append(experience)\n",
    "\n",
    "description_tags=driver.find_elements(By.XPATH,'//span[@class=\"job-desc ni-job-tuple-icon ni-job-tuple-icon-srp-description\"]')\n",
    "for i in description_tags[0:10]:\n",
    "    description=i.text\n",
    "    job_description.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d0c4636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(job_company),len(job_experience),len(job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ae9f8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mumbai, Hyderabad, Gurugram</td>\n",
       "      <td>Deloitte</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>Work youll doLiaison with on site and client t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurugram, Bengaluru</td>\n",
       "      <td>Blackbuck</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Some of the problem areas are Insights across ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...</td>\n",
       "      <td>Fort Technologies</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Who should apply 3-7 years of experience in Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...</td>\n",
       "      <td>ZS Associates</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>Experience in terms of working with cloud nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Pune, Gurugram</td>\n",
       "      <td>Times Internet</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>. A flexible and connected ZS allows us to com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>Noida</td>\n",
       "      <td>The Scholar</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>. Bachelors or Masters degree in computer scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Hyderabad, New Delhi, Pune, Gurugram, Bengaluru</td>\n",
       "      <td>Acenet</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Eligibility: Graduate with minimum 3 year expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lead Customer Success - Data Scientist</td>\n",
       "      <td>Noida, Mumbai, Pune</td>\n",
       "      <td>Searchurcollege</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Deployment mandate 1 / 2 projects atleast shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurugram, Bengaluru</td>\n",
       "      <td>Adglobal360</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "      <td>Coding experience in Java, Java Script (2-3 ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Python and ML Trainer</td>\n",
       "      <td>Varanasi, Kannur, Mumbai, New Delhi</td>\n",
       "      <td>Gartner</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>. At least 1-year of experience coding in R / ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title  \\\n",
       "0                          Data Scientist   \n",
       "1                          Data Scientist   \n",
       "2                     Lead Data Scientist   \n",
       "3                          Data Scientist   \n",
       "4                          Data Scientist   \n",
       "5                                           \n",
       "6                          Data Scientist   \n",
       "7  Lead Customer Success - Data Scientist   \n",
       "8                          Data Scientist   \n",
       "9                   Python and ML Trainer   \n",
       "\n",
       "                                            Location            Company  \\\n",
       "0                        Mumbai, Hyderabad, Gurugram           Deloitte   \n",
       "1                                Gurugram, Bengaluru          Blackbuck   \n",
       "2  Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...  Fort Technologies   \n",
       "3  Kolkata, Mumbai, New Delhi, Hyderabad, Pune, C...      ZS Associates   \n",
       "4                                     Pune, Gurugram     Times Internet   \n",
       "5                                              Noida        The Scholar   \n",
       "6    Hyderabad, New Delhi, Pune, Gurugram, Bengaluru             Acenet   \n",
       "7                                Noida, Mumbai, Pune    Searchurcollege   \n",
       "8                                Gurugram, Bengaluru        Adglobal360   \n",
       "9                Varanasi, Kannur, Mumbai, New Delhi            Gartner   \n",
       "\n",
       "  Experience                                        Description  \n",
       "0    3-6 Yrs  Work youll doLiaison with on site and client t...  \n",
       "1    3-7 Yrs  Some of the problem areas are Insights across ...  \n",
       "2    3-7 Yrs  Who should apply 3-7 years of experience in Da...  \n",
       "3    1-3 Yrs  Experience in terms of working with cloud nati...  \n",
       "4    2-4 Yrs  . A flexible and connected ZS allows us to com...  \n",
       "5    3-8 Yrs  . Bachelors or Masters degree in computer scie...  \n",
       "6    3-8 Yrs  Eligibility: Graduate with minimum 3 year expe...  \n",
       "7    4-7 Yrs  Deployment mandate 1 / 2 projects atleast shou...  \n",
       "8    3-4 Yrs  Coding experience in Java, Java Script (2-3 ye...  \n",
       "9    1-3 Yrs  . At least 1-year of experience coding in R / ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title':job_title,'Location':job_location,'Company':job_company,'Experience':job_experience,'Description':job_description})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8998015",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. \n",
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.shine.com/\n",
    " 2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field. \n",
    "3. Then click the searchbutton. \n",
    "4. Then scrape the data for the first 10 jobs results you get. \n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edcd5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cce03f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.shine.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e768dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=driver.find_element(By.XPATH,\"/html/body/div/header/div[3]/div/div/div[1]/div/input\")\n",
    "designation.click()\n",
    "time.sleep(5)\n",
    "designation=driver.find_element(By.CLASS_NAME,\"form-control  \")\n",
    "designation.send_keys(\"Data Analyst\")\n",
    "time.sleep(5)\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]/div/input\")\n",
    "location.send_keys(\"Bangalore\")\n",
    "time.sleep(3)\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[2]/div/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07ba02f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7be88b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags=driver.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard__jjUmu  white-box-border jobCard\"]/div/h2')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "company_tags=driver.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_cName__mYnow\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "    \n",
    "experience_tags=driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4dfe375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8930d299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Clinical Data Analyst</td>\n",
       "      <td>Bangalore\\n+9</td>\n",
       "      <td>sahast sales corporation</td>\n",
       "      <td>12 to 22 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore\\n+6</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>0 to 1 Yr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Azure Data engineer</td>\n",
       "      <td>Bangalore\\n+9</td>\n",
       "      <td>sahast sales corporation</td>\n",
       "      <td>12 to 22 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring For Data Analyst</td>\n",
       "      <td>Bangalore\\n+4</td>\n",
       "      <td>capgemini technology services india...</td>\n",
       "      <td>6 to 10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Executive karnataka</td>\n",
       "      <td>Bangalore\\n+12</td>\n",
       "      <td>radhika enterprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GCP Data Engineer / Developer</td>\n",
       "      <td>Bangalore\\n+9</td>\n",
       "      <td>v-tech data outsourcing</td>\n",
       "      <td>0 to 1 Yr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Clinical Data Analyst</td>\n",
       "      <td>Bangalore\\n+8</td>\n",
       "      <td>capgemini technology services india...</td>\n",
       "      <td>6 to 11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sas Programmer Analyst</td>\n",
       "      <td>Bangalore\\n+6</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>0 to 1 Yr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Operator all over India</td>\n",
       "      <td>Bangalore\\n+8</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>0 to 2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist Vacancy</td>\n",
       "      <td>Bangalore\\n+9</td>\n",
       "      <td>v-tech data outsourcing</td>\n",
       "      <td>0 to 1 Yr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title        Location  \\\n",
       "0          Clinical Data Analyst   Bangalore\\n+9   \n",
       "1                   Data Analyst   Bangalore\\n+6   \n",
       "2            Azure Data engineer   Bangalore\\n+9   \n",
       "3        Hiring For Data Analyst   Bangalore\\n+4   \n",
       "4       Data Executive karnataka  Bangalore\\n+12   \n",
       "5  GCP Data Engineer / Developer   Bangalore\\n+9   \n",
       "6          Clinical Data Analyst   Bangalore\\n+8   \n",
       "7         Sas Programmer Analyst   Bangalore\\n+6   \n",
       "8   Data Operator all over India   Bangalore\\n+8   \n",
       "9         Data Scientist Vacancy   Bangalore\\n+9   \n",
       "\n",
       "                                  Company    Experience  \n",
       "0                sahast sales corporation  12 to 22 Yrs  \n",
       "1                           techno endura     0 to 1 Yr  \n",
       "2                sahast sales corporation  12 to 22 Yrs  \n",
       "3  capgemini technology services india...   6 to 10 Yrs  \n",
       "4                     radhika enterprises    0 to 4 Yrs  \n",
       "5                 v-tech data outsourcing     0 to 1 Yr  \n",
       "6  capgemini technology services india...   6 to 11 Yrs  \n",
       "7                           techno endura     0 to 1 Yr  \n",
       "8                           techno endura    0 to 2 Yrs  \n",
       "9                 v-tech data outsourcing     0 to 1 Yr  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title':job_title,'Location':job_location,'Company':company_name,'Experience':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f115ed",
   "metadata": {},
   "source": [
    "Q3: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-15-black-128-gb/p/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W&lid=LSTMOBGTAGPTB3VS24WVZNSC6&marketplace=FLIPKART&q=iphone+15&store=tyy/4io&srno=s_1_2&otracker=AS_QueryStore_OrganicAutoSuggest_1_2_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_1_2_na_na_na&fm=organic&iid=b596c120-8114-44e8-9819-0b4dee130fa9.MOBGTAGPTB3VS24W.SEARCH&ppt=hp&ppn=homepage&ssid=s3c8i7dsvk0000001710403029549&qH=2f54b45b321e3ae5\n",
    "LIPKART\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1ed9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "608c0bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/apple-iphone-15-black-128-gb/p/itm6ac6485515ae4?pid=MOBGTAGPTB3VS24W&lid=LSTMOBGTAGPTB3VS24WVZNSC6&marketplace=FLIPKART&q=iphone+15&store=tyy/4io&srno=s_1_2&otracker=AS_QueryStore_OrganicAutoSuggest_1_2_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_1_2_na_na_na&fm=organic&iid=b596c120-8114-44e8-9819-0b4dee130fa9.MOBGTAGPTB3VS24W.SEARCH&ppt=hp&ppn=homepage&ssid=s3c8i7dsvk0000001710403029549&qH=2f54b45b321e3ae5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7147935",
   "metadata": {},
   "outputs": [],
   "source": [
    "Customer_reviews=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[2]/div/div[2]/div/div/span[2]')\n",
    "Customer_reviews.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ed39f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating=[]\n",
    "review_summary=[]\n",
    "full_review=[]\n",
    "ratings=[]\n",
    "reviewsummary=[]\n",
    "fullreviewsummary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dfea38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_fullreviews=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div[1]/div[2]/div[9]/div[7]/div/a/div')\n",
    "reviews_fullreviews.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2628c799",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]\"}\n  (Session info: chrome=122.0.6261.129); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF7CE8DAD02+56930]\n\t(No symbol) [0x00007FF7CE84F602]\n\t(No symbol) [0x00007FF7CE7042E5]\n\t(No symbol) [0x00007FF7CE7498ED]\n\t(No symbol) [0x00007FF7CE749A2C]\n\t(No symbol) [0x00007FF7CE78A967]\n\t(No symbol) [0x00007FF7CE76BCDF]\n\t(No symbol) [0x00007FF7CE7881E2]\n\t(No symbol) [0x00007FF7CE76BA43]\n\t(No symbol) [0x00007FF7CE73D438]\n\t(No symbol) [0x00007FF7CE73E4D1]\n\tGetHandleVerifier [0x00007FF7CEC56F8D+3711213]\n\tGetHandleVerifier [0x00007FF7CECB04CD+4077101]\n\tGetHandleVerifier [0x00007FF7CECA865F+4044735]\n\tGetHandleVerifier [0x00007FF7CE979736+706710]\n\t(No symbol) [0x00007FF7CE85B8DF]\n\t(No symbol) [0x00007FF7CE856AC4]\n\t(No symbol) [0x00007FF7CE856C1C]\n\t(No symbol) [0x00007FF7CE8468D4]\n\tBaseThreadInitThunk [0x00007FF9563A257D+29]\n\tRtlUserThreadStart [0x00007FF9577EAA58+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     full_review\u001b[38;5;241m.\u001b[39mappend(fullreview)\n\u001b[0;32m     21\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m next_button\u001b[38;5;241m=\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m next_button\u001b[38;5;241m.\u001b[39mclick()                      \n\u001b[0;32m     25\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    738\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    739\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]\"}\n  (Session info: chrome=122.0.6261.129); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n\tGetHandleVerifier [0x00007FF7CE8DAD02+56930]\n\t(No symbol) [0x00007FF7CE84F602]\n\t(No symbol) [0x00007FF7CE7042E5]\n\t(No symbol) [0x00007FF7CE7498ED]\n\t(No symbol) [0x00007FF7CE749A2C]\n\t(No symbol) [0x00007FF7CE78A967]\n\t(No symbol) [0x00007FF7CE76BCDF]\n\t(No symbol) [0x00007FF7CE7881E2]\n\t(No symbol) [0x00007FF7CE76BA43]\n\t(No symbol) [0x00007FF7CE73D438]\n\t(No symbol) [0x00007FF7CE73E4D1]\n\tGetHandleVerifier [0x00007FF7CEC56F8D+3711213]\n\tGetHandleVerifier [0x00007FF7CECB04CD+4077101]\n\tGetHandleVerifier [0x00007FF7CECA865F+4044735]\n\tGetHandleVerifier [0x00007FF7CE979736+706710]\n\t(No symbol) [0x00007FF7CE85B8DF]\n\t(No symbol) [0x00007FF7CE856AC4]\n\t(No symbol) [0x00007FF7CE856C1C]\n\t(No symbol) [0x00007FF7CE8468D4]\n\tBaseThreadInitThunk [0x00007FF9563A257D+29]\n\tRtlUserThreadStart [0x00007FF9577EAA58+40]\n"
     ]
    }
   ],
   "source": [
    "start=0\n",
    "end=13\n",
    "\n",
    "for page in range(start,end):\n",
    "    rating_list=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    review_list=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    fullreview_summary=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for i in rating_list:\n",
    "        ra=i.text\n",
    "        rating.append(ra)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for i in review_list:\n",
    "        review=i.text\n",
    "        review_summary.append(review)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for i in fullreview_summary:\n",
    "        fullreview=i.text\n",
    "        full_review.append(fullreview)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[1]/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]')\n",
    "    next_button.click()                      \n",
    "    time.sleep(8)\n",
    "for i in rating[0:100]:\n",
    "    rat=i\n",
    "    ratings.append(rat)\n",
    "for i in review_summary[0:100]:\n",
    "    rev=i\n",
    "    reviewsummary.append(rev)\n",
    "for i in full_review[0:100]:\n",
    "    frev=i\n",
    "    fullreviewsummary.append(frev)\n",
    "    \n",
    "print(len(ratings),len(reviewsummary),len(fullreviewsummary))\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'Rating':ratings,'Review Summary':reviewsummary,'Full Review Summary':fullreviewsummary})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(rating),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c53217",
   "metadata": {},
   "source": [
    "Q4: Scrape data forfirst 100 sneakers you find whenyouvisitflipkart.com and search for “sneakers” inthe search\n",
    "field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a484fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35fb35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6bac61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sneaker=driver.find_element(By.CLASS_NAME,\"Pke_EE\")\n",
    "sneaker.send_keys(\"sneakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e4b3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,'//button[@class=\"_2iLD__\"]')\n",
    "search.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e7986e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name=[]\n",
    "productdescription=[]\n",
    "price=[]\n",
    "brandname=[]\n",
    "prod_description=[]\n",
    "price_range=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c5bb3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Oggi Sneakers Sneakers For Men</td>\n",
       "      <td>₹558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Oggi Sneakers Sneakers For Men</td>\n",
       "      <td>₹558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>Trending Stylish Casual Outdoor Sneakers Shoes...</td>\n",
       "      <td>₹448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deals4you</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Lightweight,Comfort,Summer,Trendy,Walking,Outd...</td>\n",
       "      <td>₹339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ORICUM</td>\n",
       "      <td>Casual Shoes For Men Walking , Sneakers ,Loafe...</td>\n",
       "      <td>₹697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Oricum Lightweight Casual Shoes with High Qual...</td>\n",
       "      <td>₹3,360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Abros</td>\n",
       "      <td>Oricum Lightweight Casual Shoes with High Qual...</td>\n",
       "      <td>₹1,157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ORICUM</td>\n",
       "      <td>Classic Colourblocked Sneakers For Men</td>\n",
       "      <td>₹588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ORICUM</td>\n",
       "      <td>550 Sneakers For Men</td>\n",
       "      <td>₹704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brand                                Product_Description   Price\n",
       "0      Layasa                     Oggi Sneakers Sneakers For Men    ₹558\n",
       "1      Layasa                     Oggi Sneakers Sneakers For Men    ₹558\n",
       "2    URBANBOX  Trending Stylish Casual Outdoor Sneakers Shoes...    ₹448\n",
       "3   Deals4you                                 Sneakers For Women    ₹469\n",
       "4        aadi  Lightweight,Comfort,Summer,Trendy,Walking,Outd...    ₹339\n",
       "..        ...                                                ...     ...\n",
       "95     ORICUM  Casual Shoes For Men Walking , Sneakers ,Loafe...    ₹697\n",
       "96       PUMA  Oricum Lightweight Casual Shoes with High Qual...  ₹3,360\n",
       "97      Abros  Oricum Lightweight Casual Shoes with High Qual...  ₹1,157\n",
       "98     ORICUM             Classic Colourblocked Sneakers For Men    ₹588\n",
       "99     ORICUM                               550 Sneakers For Men    ₹704\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start=0\n",
    "end=13\n",
    "for page in range(start,end):\n",
    "    brand_list=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    product_description=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    price_list=driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in brand_list:\n",
    "        brand_name.append(i.text)\n",
    "    time.sleep(3)\n",
    "    for i in product_description:\n",
    "        productdescription.append(i.text)\n",
    "    time.sleep(3)\n",
    "    for i in price_list:\n",
    "        price.append(i.text)\n",
    "    time.sleep(3)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "for i in brand_name[0:100]:\n",
    "    bn=i\n",
    "    brandname.append(bn)\n",
    "for i in productdescription[0:100]:\n",
    "    prod=i\n",
    "    prod_description.append(prod)\n",
    "for i in price[0:100]:\n",
    "    pr=i\n",
    "    price_range.append(pr)\n",
    "    \n",
    "print(len(brandname),len(prod_description),len(price_range))\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'Brand':brandname,'Product_Description':prod_description,'Price':price_range})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fcd18",
   "metadata": {},
   "source": [
    "Q5: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU\n",
    "Type filter to “Intel Core i7” as shown in the below image:\n",
    "Aftersetting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6033a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23b046e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f0915e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "laptop.send_keys(\"Laptop\")\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\")\n",
    "search.click()\n",
    "time.sleep(10)\n",
    "\n",
    "cputype_filter=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div[2]/div/div[3]/span/div/div/div/div[6]/ul[19]/span/span[9]/li/span/a/span\")\n",
    "cputype_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d55df5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_name=[]\n",
    "ratings=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b97cea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dell Inspiron 7430 2in1 Touch Laptop, 13th Gen...</td>\n",
       "      <td></td>\n",
       "      <td>76,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP Laptop 15s, 12th Gen Intel Core i5-1235U, 1...</td>\n",
       "      <td></td>\n",
       "      <td>53,722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samsung Galaxy Book2 (NP750) Intel 12th Gen co...</td>\n",
       "      <td></td>\n",
       "      <td>48,890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSI GF63 Thin, Intel 11th Gen. i5-11260H, 40CM...</td>\n",
       "      <td></td>\n",
       "      <td>44,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acer Travelmate Business Laptop Intel Core i5-...</td>\n",
       "      <td></td>\n",
       "      <td>39,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HONOR MagicBook X16 (2024), 12th Gen Intel Cor...</td>\n",
       "      <td></td>\n",
       "      <td>46,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Acer Nitro V Gaming Laptop 13th Gen Intel Core...</td>\n",
       "      <td></td>\n",
       "      <td>78,052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dell 15 Laptop, 12th Gen Intel Core i5-1235U P...</td>\n",
       "      <td></td>\n",
       "      <td>48,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dell 15 Laptop, Intel Core i5-1135G7 Processor...</td>\n",
       "      <td></td>\n",
       "      <td>49,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASUS TUF Gaming F17, Intel Core i5-11400H 11th...</td>\n",
       "      <td></td>\n",
       "      <td>49,882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title Rating   Price\n",
       "0  Dell Inspiron 7430 2in1 Touch Laptop, 13th Gen...         76,990\n",
       "1  HP Laptop 15s, 12th Gen Intel Core i5-1235U, 1...         53,722\n",
       "2  Samsung Galaxy Book2 (NP750) Intel 12th Gen co...         48,890\n",
       "3  MSI GF63 Thin, Intel 11th Gen. i5-11260H, 40CM...         44,990\n",
       "4  Acer Travelmate Business Laptop Intel Core i5-...         39,990\n",
       "5  HONOR MagicBook X16 (2024), 12th Gen Intel Cor...         46,990\n",
       "6  Acer Nitro V Gaming Laptop 13th Gen Intel Core...         78,052\n",
       "7  Dell 15 Laptop, 12th Gen Intel Core i5-1235U P...         48,990\n",
       "8  Dell 15 Laptop, Intel Core i5-1135G7 Processor...         49,490\n",
       "9  ASUS TUF Gaming F17, Intel Core i5-11400H 11th...         49,882"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "rating=driver.find_elements(By.XPATH,'//a[@class=\"a-popover-trigger a-declarative\"]')\n",
    "price_list=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "\n",
    "for i in title_list[0:10]:\n",
    "    title_name.append(i.text)\n",
    "    \n",
    "for i in rating[0:10]:\n",
    "    ratings.append(i.text)\n",
    "\n",
    "for i in price_list[0:10]:\n",
    "    price.append(i.text)\n",
    "\n",
    "\n",
    "print(len(title_name),len(ratings),len(price))\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title':title_name,'Rating':ratings,'Price':price})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a1e72",
   "metadata": {},
   "source": [
    "Q6: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuote\n",
    "3. Than scrap a)Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f99e9aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "652efcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.azquotes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "baf224f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopQoutes_filter=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a')\n",
    "TopQoutes_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "477669a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quote=[]\n",
    "Author=[]\n",
    "TypeOfQuotes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c6a7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>TypeOfQuotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The essence of strategy is choosing what not t...</td>\n",
       "      <td>Michael Porter</td>\n",
       "      <td>Essence, Deep Thought, Transcendentalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One cannot and must not try to erase the past ...</td>\n",
       "      <td>Golda Meir</td>\n",
       "      <td>Inspiration, Past, Trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patriotism means to stand by the country. It d...</td>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>Country, Peace, War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Death is something inevitable. When a man has ...</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>Inspirational, Motivational, Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You have to love a nation that celebrates its ...</td>\n",
       "      <td>Erma Bombeck</td>\n",
       "      <td>4th Of July, Food, Patriotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Regret for the things we did can be tempered b...</td>\n",
       "      <td>Sydney J. Harris</td>\n",
       "      <td>Love, Inspirational, Motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>America... just a nation of two hundred millio...</td>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "      <td>Gun, Two, Qualms About</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>For every disciplined effort there is a multip...</td>\n",
       "      <td>Jim Rohn</td>\n",
       "      <td>Inspirational, Greatness, Best Effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The spiritual journey is individual, highly pe...</td>\n",
       "      <td>Ram Dass</td>\n",
       "      <td>Spiritual, Truth, Yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The mind is not a vessel to be filled but a fi...</td>\n",
       "      <td>Plutarch</td>\n",
       "      <td>Inspirational, Leadership, Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Quote              Author  \\\n",
       "0    The essence of strategy is choosing what not t...      Michael Porter   \n",
       "1    One cannot and must not try to erase the past ...          Golda Meir   \n",
       "2    Patriotism means to stand by the country. It d...  Theodore Roosevelt   \n",
       "3    Death is something inevitable. When a man has ...      Nelson Mandela   \n",
       "4    You have to love a nation that celebrates its ...        Erma Bombeck   \n",
       "..                                                 ...                 ...   \n",
       "995  Regret for the things we did can be tempered b...    Sydney J. Harris   \n",
       "996  America... just a nation of two hundred millio...  Hunter S. Thompson   \n",
       "997  For every disciplined effort there is a multip...            Jim Rohn   \n",
       "998  The spiritual journey is individual, highly pe...            Ram Dass   \n",
       "999  The mind is not a vessel to be filled but a fi...            Plutarch   \n",
       "\n",
       "                                 TypeOfQuotes  \n",
       "0    Essence, Deep Thought, Transcendentalism  \n",
       "1                   Inspiration, Past, Trying  \n",
       "2                         Country, Peace, War  \n",
       "3          Inspirational, Motivational, Death  \n",
       "4                4th Of July, Food, Patriotic  \n",
       "..                                        ...  \n",
       "995         Love, Inspirational, Motivational  \n",
       "996                    Gun, Two, Qualms About  \n",
       "997     Inspirational, Greatness, Best Effort  \n",
       "998                    Spiritual, Truth, Yoga  \n",
       "999      Inspirational, Leadership, Education  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start=0\n",
    "end=10\n",
    "for page in range(start,end):\n",
    "    Quote_list=driver.find_elements(By.XPATH,'//a[@class=\"title\"]') #unstrutured quote list from current webpage \n",
    "    Author_list=driver.find_elements(By.XPATH,'//div[@class=\"author\"]') #unstrutured Authors from currentwebpage \n",
    "    TypeOfQuotes_list=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]') ##unstrutured Type Of Quotes from currentwebpage \n",
    "    \n",
    "#Strutured quote list from unstrutured list\n",
    "    for i in Quote_list:\n",
    "        ql=i.text\n",
    "        Quote.append(ql)\n",
    "\n",
    "#Strutured author list from unstrutured list\n",
    "    for i in Author_list:\n",
    "        al=i.text\n",
    "        Author.append(al)\n",
    "\n",
    "#Strutured type of quote list from unstrutured list\n",
    "    for i in TypeOfQuotes_list:\n",
    "        tl=i.text\n",
    "        TypeOfQuotes.append(tl)\n",
    "\n",
    "#moving to next page\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[4]/li[12]/a')\n",
    "    next_button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "print(len(Quote),len(Author),len(TypeOfQuotes))#printing each veariale length\n",
    "\n",
    "#data frame\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'Quote':Quote,'Author':Author,'TypeOfQuotes':TypeOfQuotes})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc0902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
